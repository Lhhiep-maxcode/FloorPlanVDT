{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5605420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.patches import Rectangle\n",
    "import importlib\n",
    "import RPLAN_Toolbox.rplan.floorplan as floorplan\n",
    "import RPLAN_Toolbox.rplan.align as align\n",
    "import RPLAN_Toolbox.rplan.measure as measure\n",
    "\n",
    "\n",
    "\n",
    "importlib.reload(floorplan)   # forces reload from disk\n",
    "importlib.reload(align)   # forces reload from disk\n",
    "importlib.reload(measure)   # forces reload from disk\n",
    "\n",
    "\n",
    "from config import data_path\n",
    "from RPLAN_Toolbox.rplan.floorplan import Floorplan\n",
    "from RPLAN_Toolbox.rplan.align import align_fp_gt\n",
    "from RPLAN_Toolbox.rplan.decorate import get_dw\n",
    "from RPLAN_Toolbox.rplan.measure import compute_tf, sample_tf, compute_tf_dist\n",
    "from RPLAN_Toolbox.rplan.plot import get_figure,get_axes,plot_category,plot_boundary,plot_graph,plot_fp,plot_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2b1eed",
   "metadata": {},
   "source": [
    "# Split data for (testing) and (retrieval + training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537fad93",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \".\\\\raw_data\\\\floorplan_dataset\"\n",
    "png_file_names = [f.split('.')[0] for f in os.listdir(folder) if f.endswith('.png')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7913a7cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80788"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(png_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45650d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 8078 file names to test.txt\n",
      "Saved 72710 file names to train.txt\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(png_file_names)\n",
    "test_files = png_file_names[:int(len(png_file_names) * 0.1)]\n",
    "train_files = png_file_names[int(len(png_file_names) * 0.1):]\n",
    "\n",
    "with open(\".\\\\processed_data\\\\test.txt\", \"w\") as f:\n",
    "    for name in test_files:\n",
    "        f.write(name + \"\\n\")\n",
    "\n",
    "with open(\".\\\\processed_data\\\\train.txt\", \"w\") as f:\n",
    "    for name in train_files:\n",
    "        f.write(name + \"\\n\")\n",
    "\n",
    "print(f\"Saved {len(test_files)} file names to test.txt\")\n",
    "print(f\"Saved {len(train_files)} file names to train.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec9862e",
   "metadata": {},
   "source": [
    "# Extract .png data and save to .mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "399d3862",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_train = open('./processed_data/train.txt').read().split('\\n')\n",
    "names = names_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b6c6252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72710/72710 [1:46:14<00:00, 11.41it/s]  \n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "# point in xyxy format\n",
    "\n",
    "mat_structs = []\n",
    "\n",
    "for i in tqdm(range(len(names) - 1)):\n",
    "    file_path = f\"./raw_labeled_data/floorplan_dataset/{names[i]}.png\"\n",
    "    try:\n",
    "        fp = Floorplan(file_path)\n",
    "    except:\n",
    "        continue\n",
    "    data = fp.to_dict()\n",
    "    boxes_aligned, order, room_boundaries = align_fp_gt(data['boundary'],data['boxes'],data['types'],data['edges'])\n",
    "    data['boxes_aligned'] = boxes_aligned\n",
    "    data['order'] = order\n",
    "    data['room_boundaries'] = room_boundaries\n",
    "\n",
    "    mat_struct = {}\n",
    "    mat_struct['boundary'] = np.array(data['boundary'])\n",
    "    mat_struct['name'] = data['name']\n",
    "    mat_struct['rType'] = np.array(data['types'])\n",
    "    mat_struct['rEdge'] = np.array(data['edges'])\n",
    "    mat_struct['gtBox'] = np.array(data['boxes'])\n",
    "    mat_struct['gtBoxNew'] = np.array(data['boxes_aligned'])\n",
    "    mat_struct['order'] = np.array(data['order'])\n",
    "    mat_struct['rBoundary'] = np.array(data['room_boundaries'])\n",
    "\n",
    "    mat_structs.append(mat_struct)\n",
    "\n",
    "sio.savemat('./processed_data/data.mat', {'data': mat_structs})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053740ab",
   "metadata": {},
   "source": [
    "# From labeled images to .jpg boundary-only images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f781e885",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def labeled_png_to_boundary_jpg(png_path, save_path):\n",
    "    \"\"\"\n",
    "    Convert 4-channel labeled PNG to boundary-only JPG (black-white).\n",
    "    Boundary = black, background = white.\n",
    "    \"\"\"\n",
    "\n",
    "    img = io.imread(png_path)\n",
    "    if img is None or img.shape[2] < 3:\n",
    "        raise ValueError(f\"Image {png_path} is not a valid 4-channel labeled PNG\")\n",
    "\n",
    "    boundary_img = img[:, :, 0]\n",
    "\n",
    "    # Save as JPG\n",
    "    cv2.imwrite(save_path, boundary_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "105e6989",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8078/8078 [01:45<00:00, 76.58it/s] \n"
     ]
    }
   ],
   "source": [
    "names_test = open('./data_processed/test.txt').read().split('\\n')\n",
    "for i in tqdm(range(len(names_test) - 1)):\n",
    "    labeled_png_to_boundary_jpg(f\"./data_raw_labeled/floorplan_dataset/{names_test[i]}.png\", f\"./data_test/{names_test[i]}.jpg\")\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80137b63",
   "metadata": {},
   "source": [
    "# 1.tf_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7e35f3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 2204.40it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 6580.33it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2649.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "data = sio.loadmat(data_path, squeeze_me=True, struct_as_record=False)['data']\n",
    "data_dict = {d.name:d for d in data}\n",
    "\n",
    "names_train = open('./data/train.txt').read().split('\\n')[:10]\n",
    "names_test = open('./data/test.txt').read().split('\\n')[:10]\n",
    "n_train = len(names_train)\n",
    "n_test = len(names_test)\n",
    "\n",
    "# turning function: training data\n",
    "trainTF = []\n",
    "tf_train = []\n",
    "for i in tqdm(range(n_train)):\n",
    "    boundary = data_dict[names_train[i]].boundary\n",
    "    x,y = compute_tf(boundary)\n",
    "    trainTF.append({'x':x,'y':y})\n",
    "pickle.dump(trainTF,open('./data/trainTF.pkl','wb'))\n",
    "\n",
    "tf_train = []\n",
    "for i in tqdm(range(n_train)):\n",
    "    x,y = trainTF[i]['x'],trainTF[i]['y']\n",
    "    tf_train.append(sample_tf(x,y))\n",
    "tf_train = np.stack(tf_train,axis=0)\n",
    "np.save('./data/tf_train.npy',tf_train)\n",
    "      \n",
    "# turning function: testing data                   \n",
    "testTF = []\n",
    "for i in tqdm(range(n_test)):\n",
    "    boundary = data_dict[names_test[i]].boundary\n",
    "    x,y = compute_tf(boundary)\n",
    "    testTF.append({'x':x,'y':y})\n",
    "pickle.dump(testTF,open('./data/testTF.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9a562710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing turning function distance ... it will take a long time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 1221.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# turning function distance: test-train\n",
    "print('Computing turning function distance ... it will take a long time.')\n",
    "D_test_train = np.zeros((n_test,n_train),dtype='float32')\n",
    "for i in tqdm(range(n_test)):\n",
    "    for j in range(n_train):\n",
    "        D_test_train[i,j] = compute_tf_dist(testTF[i],trainTF[j])\n",
    "np.save('./data/D_test_train.npy',D_test_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da105911",
   "metadata": {},
   "source": [
    "# 2.data_train_converted.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "99b215b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 85598.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "data = sio.loadmat(data_path, squeeze_me=True, struct_as_record=False)['data']\n",
    "data_dict = {d.name:d for d in data}\n",
    "\n",
    "names_train = open('./data/train.txt').read().split('\\n')[:10]\n",
    "n_train = len(names_train)\n",
    "\n",
    "trainTF = pickle.load(open('./data/trainTF.pkl','rb'))\n",
    "\n",
    "data_converted = []\n",
    "\n",
    "for i in tqdm(range(n_train)):\n",
    "    d = data_dict[names_train[i]]\n",
    "    d_converted = {}\n",
    "    d_converted['name'] = d.name\n",
    "    d_converted['boundary'] = d.boundary\n",
    "    d_converted['boxes_aligned'] = np.concatenate([d.boxes_aligned,d.types[:,None]],axis=-1)\n",
    "    d_converted['order'] = d.order\n",
    "    d_converted['edges'] = d.edges\n",
    "    d_converted['room_boundaries'] = d.room_boundaries\n",
    "    data_converted.append(d_converted)\n",
    "\n",
    "sio.savemat('./data/data_train_converted.mat',{'data':data_converted,'nameList':names_train,'trainTF':trainTF})\n",
    "data = sio.loadmat('./data/data_train_converted.mat', squeeze_me=True, struct_as_record=False)\n",
    "pickle.dump(data,open('./data/data_train_converted.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf22e22a",
   "metadata": {},
   "source": [
    "# 3.rNum_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5888f0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 11060.93it/s]\n"
     ]
    }
   ],
   "source": [
    "data = sio.loadmat(data_path, squeeze_me=True, struct_as_record=False)['data']\n",
    "data_dict = {d.name:d for d in data}\n",
    "\n",
    "names_train = open('./data/train.txt').read().split('\\n')[:10]\n",
    "n_train = len(names_train)\n",
    "\n",
    "rNum = np.zeros((n_train,14),dtype='uint8')     # 13 rooms, each floorplan in each row\n",
    "for i in tqdm(range(n_train)):\n",
    "    rType = data_dict[names_train[i]].types\n",
    "    for j in range(13):\n",
    "        rNum[i,j] = (rType==j).sum()\n",
    "    \n",
    "    # count the number of important rooms\n",
    "    rNum[i,13] = rNum[i,[1,5,6,7,8]].sum()\n",
    "\n",
    "np.save('./data/rNum_train.npy',rNum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda2ea32",
   "metadata": {},
   "source": [
    "# 4.data_train_eNum.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cc6eb4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 17063.89it/s]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load converted training data (a list of dicts, each floorplan as one dict)\n",
    "data = pickle.load(open('./data/data_train_converted.pkl','rb'))['data']\n",
    "\n",
    "# Read list of training names\n",
    "names_train = open('./data/train.txt').read().split('\\n')[:10]\n",
    "n_train = len(names_train)\n",
    "\n",
    "# Initialize storage for edge statistics\n",
    "# Shape: (n_train, 25) because each floorplan produces a 5x5 adjacency matrix flattened to 25 values\n",
    "eNum = np.zeros((n_train,25),dtype='uint8')\n",
    "\n",
    "for i in tqdm(range(n_train)):\n",
    "    d = data[i]\n",
    "\n",
    "    # rType: room type IDs (last column of box array)\n",
    "    rType = d.boxes_aligned[:,-1]\n",
    "\n",
    "    # eType: the room types at both ends of each edge\n",
    "    eType = rType[d.edges[:,:2]]\n",
    "\n",
    "    # === Map raw room types into coarser categories ===\n",
    "    # This array remaps 18 room types into 10 categories (original MATLAB indexing was 1-based)\n",
    "    rMap = np.array([1,2,3,4,1,2,2,2,2,5,1,6,1,10,7,8,9,10])-1 \n",
    "\n",
    "    # Apply mapping\n",
    "    edge = rMap[eType]\n",
    "\n",
    "    # Reorder into 6 final categories: [0..5]\n",
    "    reorder = np.array([0,1,3,2,4,5])\n",
    "    edge = reorder[edge]\n",
    "\n",
    "    # Only keep edges where both endpoints are between 1–5 (valid categories)\n",
    "    I = (edge[:,0]<=5)&(edge[:,0]>=1)&(edge[:,1]<=5)&(edge[:,1]>=1)\n",
    "    edge = edge[I,:]-1  # convert to 0-based index\n",
    "\n",
    "    # Initialize adjacency matrix for this floorplan (5x5 categories)\n",
    "    e = np.zeros((5,5),dtype='uint8') \n",
    "\n",
    "    # Count edges between categories\n",
    "    for j in range(len(edge)):\n",
    "        e[edge[j,0],edge[j,1]] += 1\n",
    "        if edge[j,0] != edge[j,1]:   # if different categories, make symmetric\n",
    "            e[edge[j,1],edge[j,0]] += 1\n",
    "    \n",
    "    # Flatten 5x5 adjacency into length-25 vector\n",
    "    eNum[i] = e.reshape(-1)\n",
    "\n",
    "# Save results: edge connection statistics for all training floorplans\n",
    "pickle.dump({'eNum':eNum},open('./data/data_train_eNum.pkl','wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33c9cdd",
   "metadata": {},
   "source": [
    "# 5.data_test_converted.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a331e35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 19897.08it/s]\n"
     ]
    }
   ],
   "source": [
    "data = sio.loadmat(data_path, squeeze_me=True, struct_as_record=False)['data']\n",
    "data_dict = {d.name:d for d in data}\n",
    "testTF = pickle.load(open('./data/testTF.pkl','rb'))\n",
    "rNum = np.load('./data/rNum_train.npy')\n",
    "\n",
    "names_train = open('./data/train.txt').read().split('\\n')[:10]\n",
    "names_test = open('./data/test.txt').read().split('\\n')[:10]\n",
    "n_train = len(names_train)\n",
    "n_test = len(names_test)\n",
    "\n",
    "D = np.load('./data/D_test_train.npy')\n",
    "data_converted = []\n",
    "for i in tqdm(range(n_test)):\n",
    "    d = data_dict[names_test[i]]\n",
    "    d_converted = {}\n",
    "    d_converted['boundary'] = d.boundary\n",
    "    d_converted['tf'] = testTF[i]\n",
    "    topK = np.argsort(D[i])[:1000]\n",
    "    d_converted['topK'] = topK\n",
    "    d_converted['topK_rNum'] = rNum[topK]\n",
    "    data_converted.append(d_converted)\n",
    "\n",
    "sio.savemat('./data/data_test_converted.mat',{'data':data_converted,'testNameList':names_test,'trainNameList':names_train})\n",
    "data = sio.loadmat('./data/data_test_converted.mat', squeeze_me=True, struct_as_record=False)\n",
    "pickle.dump(data,open('./data/data_test_converted.pkl','wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcfab04",
   "metadata": {},
   "source": [
    "# 6.cluster.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c7dbb2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 10894.30it/s]\n",
      "WARNING clustering 10 points to 5 centroids: please provide at least 195 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering 10 points in 1000D to 5 clusters, redo 1 times, 200 iterations\n",
      "  Preprocessing in 0.00 s\n",
      "  Iteration 199 (0.25 s, search 0.05 s): objective=3214.41 imbalance=1.300 nsplit=0       \n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "tf_train = pickle.load(open('./data/trainTF.pkl','rb'))\n",
    "\n",
    "tf = []\n",
    "for i in tqdm(range(len(tf_train))):\n",
    "    tf_i = tf_train[i]\n",
    "    tf.append(sample_tf(tf_i['x'],tf_i['y']))\n",
    "\n",
    "d = 1000\n",
    "tf = np.array(tf).astype(np.float32)\n",
    "\n",
    "ncentroids = 5\n",
    "niter = 200\n",
    "verbose = True\n",
    "\n",
    "kmeans = faiss.Kmeans(d, ncentroids, niter=niter, verbose=verbose,gpu=False)\n",
    "kmeans.train(tf)\n",
    "centroids = kmeans.centroids\n",
    "\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(tf)\n",
    "nNN = 1000\n",
    "D, I = index.search (kmeans.centroids, nNN)\n",
    "\n",
    "np.save(f'./data/centroids_train.npy',centroids)\n",
    "np.save(f'./data/clusters_train.npy',I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "eb0c6935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.       , 0.       , 0.       , ..., 6.2831855, 6.2831855,\n",
       "        6.2831855],\n",
       "       [0.       , 0.       , 0.       , ..., 6.2831855, 6.2831855,\n",
       "        6.2831855],\n",
       "       [0.       , 0.       , 0.       , ..., 6.2831855, 6.2831855,\n",
       "        6.2831855],\n",
       "       [0.       , 0.       , 0.       , ..., 6.2831855, 6.2831855,\n",
       "        6.2831855],\n",
       "       [0.       , 0.       , 0.       , ..., 6.2831855, 6.2831855,\n",
       "        6.2831855]], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "43b424de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1000)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c87845e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph2plan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
